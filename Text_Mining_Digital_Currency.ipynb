{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Mining - Digital Currency",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramadhan1212/Text-Mining-Fast-Edu/blob/main/Text_Mining_Digital_Currency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-mrTVCfmbpX"
      },
      "source": [
        "## **Text Mining**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6v1llWX1ybs"
      },
      "source": [
        "### **Install and Load Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orcAloXKEtUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed865f6-ebd5-4e26-8b75-67248ccd7423"
      },
      "source": [
        "# Install packages\n",
        "! pip install tweet-preprocessor\n",
        "! pip install pyLDAvis"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 20.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.5)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.8.1)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.6)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyLDAvis) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyLDAvis) (3.0.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=1bdf7f1cc1360569ec8d97a180c283832de50d4ec479e6c12f00fe57b10ced2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.17 pyLDAvis-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFNTlh7j0yW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bbcbdf9-f595-449f-90da-165d873c7702"
      },
      "source": [
        "# Load packages\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import preprocessor as p\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import wordcloud\n",
        "import nltk\n",
        "import warnings\n",
        "import itertools\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n",
            "/usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Mapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0WPn6T5IYLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8483c59a-1486-422c-a740-a78d3a9caea7"
      },
      "source": [
        "# Import module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
        "from tqdm import tqdm\n",
        "from nltk import bigrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-TGl07g1GR_"
      },
      "source": [
        "# Set parameter\n",
        "warnings.filterwarnings('ignore')\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POd_N1Vx18Ft"
      },
      "source": [
        "### **Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX4JNHdFmLbd"
      },
      "source": [
        "# Import data\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/andrybrew/bi-8-maret/main/data/tweet-full.csv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9lNfxTM1wV0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a809b8c7-2d38-4b2d-901a-c2a0e2f3f666"
      },
      "source": [
        "# Lihat 5 baris pertama data\n",
        "df.head(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   created_at      screen_name  \\\n",
              "0  2020-12-30    PapaPapag1954   \n",
              "1  2020-12-30  HsarafattoHogan   \n",
              "2  2020-12-30       hunter_1st   \n",
              "3  2020-03-26       hunter_1st   \n",
              "4  2020-12-30       Deanmufc77   \n",
              "\n",
              "                                                text  \n",
              "0  @LewisECFC @Kes1977 @herotroyippygod @GlobeSen...  \n",
              "1  @davidmcw Come on David, tell me billionaires ...  \n",
              "2  @PeterSchiff Chinas digital currency could bri...  \n",
              "3  @Bipoker @woofBIGDAWG @PeterSchiff Bitcoin is ...  \n",
              "4  @ecb @finanzfluss And people still call the NW...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b33a54fe-a524-4829-9008-0eb17250dc7b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>screen_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>PapaPapag1954</td>\n",
              "      <td>@LewisECFC @Kes1977 @herotroyippygod @GlobeSen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>HsarafattoHogan</td>\n",
              "      <td>@davidmcw Come on David, tell me billionaires ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>hunter_1st</td>\n",
              "      <td>@PeterSchiff Chinas digital currency could bri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-03-26</td>\n",
              "      <td>hunter_1st</td>\n",
              "      <td>@Bipoker @woofBIGDAWG @PeterSchiff Bitcoin is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>Deanmufc77</td>\n",
              "      <td>@ecb @finanzfluss And people still call the NW...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b33a54fe-a524-4829-9008-0eb17250dc7b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b33a54fe-a524-4829-9008-0eb17250dc7b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b33a54fe-a524-4829-9008-0eb17250dc7b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfntSX4DopwQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e82684-0955-4f1a-b9ad-b960238874a7"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 26768 entries, 0 to 26767\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   created_at   26768 non-null  object\n",
            " 1   screen_name  26768 non-null  object\n",
            " 2   text         26768 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 627.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIXInlqooGaO"
      },
      "source": [
        "### **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PtaspkU8ei5"
      },
      "source": [
        "# Pilih 5 kolom teks saja\n",
        "tweet = df[['text']]\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmpAtGBFFGCG"
      },
      "source": [
        "### **Transformation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewjn6JMbDejZ"
      },
      "source": [
        "# Membuat fungsi transformasi tweet\n",
        "def transform_tweet(row):\n",
        "  tweet = row['text']\n",
        "  tweet = p.clean(tweet)\n",
        "  tweet = str.lower(tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMa_w71GEaFS"
      },
      "source": [
        "# Mengaplikasikan fungsi transofrmasi\n",
        "tweet['transformed'] = tweet.apply(transform_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0ZiliHSFSnH"
      },
      "source": [
        "### **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0l9wvMPFpNE"
      },
      "source": [
        "# Download Punkt\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJfyNe2VC6rN"
      },
      "source": [
        "# Membuat fungsi tokenization\n",
        "def tokenize_tweet(row):\n",
        "    tweet = row['transformed']\n",
        "    tokens = nltk.word_tokenize(tweet)\n",
        "    token_words = [w for w in tokens if w.isalpha()]\n",
        "    return token_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFmjyoGi_rxN"
      },
      "source": [
        "# Mengaplikasikan fungsi tokenization\n",
        "tweet['tokenized'] = tweet.apply(tokenize_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtKXiR7YF9RZ"
      },
      "source": [
        "### **Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNKhzkZvF7mR"
      },
      "source": [
        "# Download Wordnet\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXSeJu9gGK5e"
      },
      "source": [
        "# Membuat fungsi lemmatization\n",
        "def lemmatize_tweet(row):\n",
        "    list = row['tokenized']\n",
        "    lemmatize_list = [WordNetLemmatizer().lemmatize(w, pos='v') for w in list]\n",
        "    return(lemmatize_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZm2bfvaGjyu"
      },
      "source": [
        "# Mengaplikasikan fungsi lemmatization\n",
        "tweet['lemmatized'] = tweet.apply(lemmatize_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zeuh1xNYIzy5"
      },
      "source": [
        "### **Stopword Removal**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7XcHI7VIwjW"
      },
      "source": [
        "# Download stopword bahasa inggris\n",
        "nltk.download('stopwords')\n",
        "stops = set(stopwords.words(\"english\"))     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUidpdOBuBU"
      },
      "source": [
        "# Membuat fungsi lemmatization\n",
        "def stopword_tweet(row):\n",
        "    list = row['lemmatized']\n",
        "    stopword_list = [w for w in list if not w in stops]\n",
        "    return(stopword_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPei0h4uCEh5"
      },
      "source": [
        "# Mengaplikasikan fungsi Stopword\n",
        "tweet['stopword'] = tweet.apply(stopword_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAkl9x-LI4HL"
      },
      "source": [
        "### **Rejoin**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGzqcONlCHYo"
      },
      "source": [
        "# Membuat fungsi rejoin\n",
        "def rejoin_tweet(row):\n",
        "    list = row['stopword']\n",
        "    joined_words = ( \" \".join(list))\n",
        "    return joined_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GkNjo6bCc36"
      },
      "source": [
        "# Mengaplikasikan fungsi rejoin\n",
        "tweet['final'] = tweet.apply(rejoin_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OkxLRK7JwRZ"
      },
      "source": [
        "# Final tweet yang sudah di proses\n",
        "tweet_clean = tweet[['final']]\n",
        "tweet_clean = tweet_clean.rename(columns={'final': 'text'})\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet_clean.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_YvObORoOm8"
      },
      "source": [
        "### **Wordcloud**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmiFeAwzoOK2"
      },
      "source": [
        "# Visualisasi Word Cloud\n",
        "text_wordcloud = \" \".join(tweet for tweet in tweet_clean.text)\n",
        "\n",
        "cloud = WordCloud(background_color='white').generate(text_wordcloud)\n",
        "\n",
        "plt.figure(figsize=(10, 10), facecolor=None)\n",
        "plt.imshow(cloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VylHW2_Um_eo"
      },
      "source": [
        "### **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqUKHLCim63Q"
      },
      "source": [
        "# Download corpus untuk sentiment analysis\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWHhRbz0nJ6J"
      },
      "source": [
        "# Sentiment Analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "listy = [] \n",
        "for index, row in tweet_clean.iterrows():\n",
        "  ss = sid.polarity_scores(row['text'])\n",
        "  listy.append(ss)\n",
        "  \n",
        "se = pd.Series(listy)\n",
        "tweet_clean['polarity'] = se.values\n",
        "display(tweet_clean.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE7V6DNsMf9p"
      },
      "source": [
        "# Visualisasi Pie Chart\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "sizes  = [ss['neg'], ss['neu'], ss['pos']]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "plt.axis('equal') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvLdU2GGoY_d"
      },
      "source": [
        "### **Topic Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSyZiXaiNI3P"
      },
      "source": [
        "# clone tambahan library dari github\n",
        "! git clone https://github.com/machine-learning-ss/tm\n",
        "\n",
        "# Set Data Directory\n",
        "os.chdir('tm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7sbfDlWNJUp"
      },
      "source": [
        "import MyLib as TS\n",
        "\n",
        "Tweets = tweet_clean['text']\n",
        "print('Total loaded tweets = {0}'.format(len(Tweets)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPBZxuW0NYoH"
      },
      "source": [
        "n_topics = 4\n",
        "top_topics = 4\n",
        "top_words = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_mznEgLNvK2"
      },
      "source": [
        "# Feature Extraction\n",
        "count_vector = CountVectorizer(token_pattern = r'\\b[a-zA-Z]{3,}\\b') \n",
        "dtm_tf = count_vector.fit_transform(Tweets)\n",
        "tf_terms = count_vector.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1NgY3twNwBB"
      },
      "source": [
        "# Fungsi untuk mencari topic\n",
        "lda_tf = LatentDirichletAllocation(n_components=n_topics, learning_method='online', random_state=0).fit(dtm_tf)\n",
        "\n",
        "# Menampilkan Topik\n",
        "vsm_topics = lda_tf.transform(dtm_tf); doc_topic =  [a.argmax()+1 for a in tqdm(vsm_topics)] # topic of docs\n",
        "print('In total there are {0} major topics, distributed as follows'.format(len(set(doc_topic))))\n",
        "plt.hist(np.array(doc_topic), alpha=0.5); plt.show()\n",
        "print('Printing top {0} Topics, with top {1} Words:'.format(top_topics, top_words))\n",
        "TS.print_Topics(lda_tf, tf_terms, top_topics, top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOcvEGgENy0f"
      },
      "source": [
        "# Visualisasi Topic Secara Interaktif\n",
        "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, count_vector) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B07sph-yRgrI"
      },
      "source": [
        "### **Text Network Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Ue_Hm2R-vV"
      },
      "source": [
        "# Pilih teks\n",
        "text = tweet_clean['text']\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj-AW6a3SP3c"
      },
      "source": [
        "# Tokenize\n",
        "text_data = [word_tokenize(i) for i in text]\n",
        "print(text_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nqqlH4NTgyK"
      },
      "source": [
        "# Membuat fungsi cooccurence\n",
        "def generate_co_occurrence_matrix(corpus):\n",
        "    vocab = set(corpus)\n",
        "    vocab = list(vocab)\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "    bi_grams = list(bigrams(corpus))\n",
        "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
        " \n",
        "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
        " \n",
        "    for bigram in bigram_freq:\n",
        "        current = bigram[0][1]\n",
        "        previous = bigram[0][0]\n",
        "        count = bigram[1]\n",
        "        pos_current = vocab_index[current]\n",
        "        pos_previous = vocab_index[previous]\n",
        "        co_occurrence_matrix[pos_current][pos_previous] = count\n",
        "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
        " \n",
        "    return co_occurrence_matrix, vocab_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJnRvQEuTwPz"
      },
      "source": [
        "# Membuat Adjacency Matrix\n",
        "data = list(itertools.chain.from_iterable(text_data))\n",
        "matrix, vocab_index = generate_co_occurrence_matrix(data)\n",
        " \n",
        " \n",
        "data_matrix = pd.DataFrame(matrix, index=vocab_index,\n",
        "                             columns=vocab_index)\n",
        "\n",
        "# Show Adjacency Matrix\n",
        "data_matrix.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9zsB-9qT0sA"
      },
      "source": [
        "# Membuat Network dengan Adjacency Matrix\n",
        "G = nx.from_pandas_adjacency(data_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbJuj86QZrwr"
      },
      "source": [
        "# Degree Centrality\n",
        "degree = nx.degree_centrality(G)\n",
        "\n",
        "list_node = list(degree) \n",
        "selected_node = list_node[1:600]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYCBGhCoT5ym"
      },
      "source": [
        "sampled_graph = G.subgraph(selected_node)\n",
        "\n",
        "plt.figure(figsize=(10, 10), facecolor=None)\n",
        "nx.draw(sampled_graph, with_labels=True, \n",
        "        node_color='skyblue', node_size=600, \n",
        "        arrowstyle='->',arrowsize=20, edge_color='r',\n",
        "        font_size=7,\n",
        "        pos=nx.kamada_kawai_layout(sampled_graph))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}